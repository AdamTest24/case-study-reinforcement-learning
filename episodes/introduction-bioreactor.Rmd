---
title: 'Deep Q network for bioreactor optimisation'
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- How can a Reinforcement Learning agent learn bioreactor environment that can handle an arbitrary number of bacterial strains?
- Why the RL agent is robust to different initial conditions and targets?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Learn how to use Deep Q network to control of multiple interacting species in a bioreactor environment.

::::::::::::::::::::::::::::::::::::::::::::::::

## Introduction

In recent years, synthetic biology and industrial bioprocessing have been implementing increasingly complex systems composed of multiple, interacting [microbial strains](https://en.wikipedia.org/wiki/Strain_(biology)). 
This has many advantages over single culture systems, including enhanced modularization and the reduction of the metabolic burden imposed on strains. Despite these advantages, the control of multi-species communities (co-cultures) within bioreactors remains extremely challenging and this is the key reason why most industrial processing still uses single cultures. Some of the challenges are competitive exclusion, the long-term prediction is challenging due to genetic drift [5] and difficulty to produce stable systems as variables increase [6]. Hence, the following case study demonstrate the efficacy of using reinforcement learning to control co-cultures within continous bioreactors [(Treloar et al. 2020)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007783).

### Reinforcement learning to control two auxotrophic species in a chemostat.

Reinforcement learning is a branch of machine learning concerned with optimising an agent’s behaviour within an environment. 
The agent learns an optimal behaviour policy by observing environmental states and selecting from a set of actions that change the environment’s state (Fig 1A). 
The agent learns to maximise an external reward that is dependent on the state of the environment. 
The training of a reinforcement learning agent is often broken up into episodes. 
An **episode** is defined as a temporal sequence of states, rewards and corresponding actions (generated by the agent interacting with the environment) until a terminal state is reached. The total reward obtained during an episode is called **the return**. 
For this work, we used a data-efficient variant of reinforcement learning called Neural Fitted Q-learning [26–28].

Fig 1 (A) The basic reinforcement learning loop; the agent interacts with its environment through actions and observes the state of the environment along with a reward. The agent acts to maximise the total reward it receives (the return).

Fig 1 (B) System of two auxotrophs ($N_1$ and $N_2$) dependent on two different auxotrophic nutrients nutrients arginine and tryptophan ($C_1$ and $C_2$), with competition over a common carbon source, glucose, ($C_0$).

Fig 1 (C) Diagram of a chemostat, also knows as [biorector](https://en.wikipedia.org/wiki/Bioreactor). The state observed by the reinforcement learning agent is composed of the populations of two strains of bacteria; the actions taken by the agent control the concentration of auxotrophic nutrients flowing into the reactor.

Fig 1 (D) Representative system trajectory. The agent’s actions, taken at discrete time-points (blue circles), influence the state dynamics (black arrows), with the aim of fulfilling the reward condition (moving to the centre of the green circle). The state is comprised of the (continuously-defined) abundance of two microbial populations, $N_1$ and $N_2$. 
The agent’s actions dictate the rate at which auxotrophic nutrients flow into the reactor. At each time-step, the agent’s reward is dependent on the distance between the current state and the target state.

![fig](fig/rl-chemostat.png)   
**Figure 1.** RL for the control of two auxotrophic species in a chemostat.


## Setting up DQN agent
### QNetwork
Let's start with the creation of QNetwork using pytorch's neural network modules, using a simple feed-forward network with two hidden layers [27].

```python
class QNetwork(nn.Module):
    """Represent the agent's policy model"""
    
    def __init__(self, state_size, action_size, layer1_size=64, layer2_size=64):
        """Build a network that can take a description of an environment's state and 
        output the value of available actions.
        
        Params
        =======
            state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            layer1_size (int): Number of nodes in first hidden layer
            layer2_size (int): Number of nodes in second hidden layer
        """
        super(QNetwork, self).__init__() ## calls __init__ method of nn.Module class
        self.layer1 = nn.Linear(state_size, layer1_size)
        self.layer2 = nn.Linear(layer1_size, layer2_size)
        self.layer3 = nn.Linear(layer2_size, action_size)

    def forward(self, x):
        """Map state -> action values."""
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)
```

### Memory

For classification tasks each sample data require to be independent to any other data samples. 
However for reinforcement learning the training samples are temporarily correlated and **not** drawn from stationary distributions. 
Hence, the `ReplayerBuffer` method stores experiences of the agent to create mini-batches of training data from randomly sample experiences. 
`ReplayerBuffer` use helpers from Python's `collections` module: (1) `deque`, similar to `list` that set maximum length using `buffer_size` and (2) `namedtuple` which makes `tuple` more readable using name fields instead of numeric indexes.

```python
class ReplayBuffer:
    """Fixed-size buffer to store experience tuples"""
    
    def __init__(self, action_size, buffer_size, batch_size):
        """Initialize a ReplayBuffer object

        Params
        ======
            action_size (int): dimension of each action
            buffer_size (int): maximum size of buffer
            batch_size (int): size of each training batch
        """

        self.action_size = action_size
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        self.experience = namedtuple("Experience", field_names=["state",
                                                                "action",
                                                                "reward",
                                                                "next_state",
                                                                "done"])

    def add(self,state, action, reward, next_state,done):
        """Add a new experience to memory"""
        e = self.experience(state, action, reward, next_state, done)
        self.memory.append(e)

    def sample(self):
        """Randomly sample a batch of experiences from memory"""
        experiences = random.sample(self.memory, k=self.batch_size)

        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)
        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)
        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)

        return (states, actions, rewards, next_states, dones)

    def __len__(self):
        """Return the current size of internal memory"""
        return len(self.memory)
```

### Configuration of the DQN_agent()
We can configure how the agent learns from its environment with the following variables:

* The variable `TAU` enable us to perform soft updates on the parameters of the $Q_{target}$ network, so that they shift towards the $Q$ network parameters incrementally rather than duplicate them at a single time step (e.g., 0.001).  
* The variable `UPDATE_EVERY` relates to the frequency with which we perform both the gradient descent step on the $Q$ network parameters and the soft update of the $Q_{target}$ parameters (e.g., 4).  
* The variable `BUFFER_SIZE` set replay buffer size (e.g., 100000).  
* The variable `BATCH_SIZE` for minibatch size (e.g., 64).  
* The variable `GAMMA` is discount factor (e.g., 0.99). `GAMMA` represents how heavily the possible future rewards weigh in on decisions (0.9 is a common choice in RL).  
* The variable `LR` is the learning rate (e.g., 0.0005).


The neural network, $QNetwork$, is used to estimate **the value function**. 
$QNetwork$ consisting of 20 nodes [27], 64 nodes in this example (or any number depending on available computing power)
The input layer had $n$ nodes, one for each **microbial strain**; the linear output layer had $2^n$ nodes corresponding to each **variable action**.
Adam optimiser is used to dynamically adapt the learning rate which is common in RL. 
The **populations levels** were scaled by a factor of `10e-5` before being entered the neural network  and generated values between 0 to 1 and prevent instability.

#### Initialisation
`DQN_agent` contains two instances of `Qnetwork`: $Q$ and $Q_{target}$ and adam optimiser for $Q$ network which copies parameters to $Q_{target}$ network and replay memory with `ReplayerBuffer()` class.

#### `get_explore_rate`
`get_explore_rate` computes the epsilon in the agent's epsilon-greedy policy based on the current episode, `episode`, and decay value that controls the rate of decay of the explore rate.

#### `epsilon_greedy_policy`
`epsilon_greedy_policy` returns an action for given the current state and the epsilon-greedy action selection.  
`epsilon_greedy_policy` is used to randomly choose an action with probabilty `epsilon` and the action $max_{a}Q(s_{t}, a)$ with probability `1-epsilon`.

#### `update_target(self, model, target_model)`
Update target model parameters with weights will be copied from local model  to target model

#### `update_Q(self, experiences)`
`update_Q` function train our Q network with a sample of experiences from the agent's memory using the discount factor, `GAMMA`.

#### `train(self, n_episodes=200, max_t=1000, decay=None, verbose=True)`
Then we have our training function, adding and sampling from the agent's memory.

```python
class DQN_agent():
    """Agent that interacts with and learns from an environment using artificial neural networks 
    to approximate its state-action value function"""

    def __init__(self, 
                 env, 
                 state_size, 
                 action_size,
                 BUFFER_SIZE = int(1e5),
                 BATCH_SIZE = 64,
                 GAMMA = 0.99,
                 TAU = 1e-3,
                 LR = 5e-4,
                 UPDATE_EVERY = 4):
        """Initialize an Agent object

        Params
        =======
            env: an environment object
            state_size (int): dimension of each state
            action_size (int): dimension of each action
            BUFFER_SIZE = int(1e5)  # replay buffer size
            BATCH_SIZE = 64         # minibatch size
            GAMMA = 0.99            # discount factor
            TAU = 1e-3              # for soft update of target parameters
            LR = 5e-4               # learning rate
            UPDATE_EVERY = 4        # how often to update the network
        """

        self.env = env
        self.state_size = state_size
        self.action_size = action_size
        self.BUFFER_SIZE = BUFFER_SIZE
        self.BATCH_SIZE = BATCH_SIZE
        self.GAMMA = GAMMA
        self.TAU = TAU
        self.LR = LR
        self.UPDATE_EVERY = UPDATE_EVERY
        
        # Function approximation networks:
        self.q_network = QNetwork(state_size, action_size).to(device)
        self.q_network_target = QNetwork(state_size, action_size).to(device)

        # Optimise the parameters in the Q network, using the learning rate defined above
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.LR)

        # Replay memory
        self.memory = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE)
        # Initialize time step (for updating every UPDATE_EVERY steps)
        self.t_step = 0

    def get_explore_rate(self, episode, decay):
        """Calculates the logarithmically decreasing explore rate

        Params
        ======
            episode (int): the current episode
            decay (float): controls the rate of decay of the explore rate
        
        Returns
        =======
            explore_rate (float): the epsilon in the agent's epsilon-greedy policy
        """

        # Input validation
        if not 0 < decay:
            raise ValueError("decay needs to be above 0")
        
        # Ensure rate returned is between 0 and 1:
        min_explore_rate = 0
        max_explore_rate = 1
        explore_rate = 1.0 - math.log10(episode / decay)
        return max(min_explore_rate, min(max_explore_rate, explore_rate))
    
    def policy(self, state, epsilon=0):
        """Returns action for given state as per current policy

        Params
        ======
            state (array_like): current state
            epsilon (float): for epsilon-greedy action selection
        """

        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        self.q_network.eval()
        with torch.no_grad():
            action_values = self.q_network(state)
        self.q_network.train()

        # Epsilon-greedy action selction
        if random.random() > epsilon:
            return np.argmax(action_values.cpu().data.numpy())
        else:
            return random.choice(np.arange(self.action_size))

    def update_target(self, model, target_model):
        """Update target model parameters

        Params
        =======
            local model (PyTorch model): weights will be copied from
            target model (PyTorch model): weights will be copied to
        """
        for target_param, local_param in zip(target_model.parameters(), model.parameters()):
            target_param.data.copy_(self.TAU*local_param.data + (1-self.TAU)*target_param.data) 

    def update_Q(self, experiences):
        """Update value parameters using given batch of experience tuples.
        
        Params
        =======
            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples
        """
        states, actions, rewards, next_states, dones = experiences
        
        # We use mean squared error as the loss function
        criterion = torch.nn.MSELoss()
        # The local model is the one we need to train so we put it in training mode
        self.q_network.train()
        # Conversely, we want the target model to be in evaluation mode so that when 
        # we do a forward pass it does not calculate the gradients
        self.q_network_target.eval()
        
        with torch.no_grad():
            future_pred = self.q_network_target(next_states).detach().max(1)[0].unsqueeze(1)

        # .detach() ->  Returns a new Tensor, detached from the current graph.
        targets = rewards + (self.GAMMA * future_pred * (1 - dones))

        # Shape of output from the model (batch_size, action_size) 
        predicted_targets = self.q_network(states).gather(1, actions)

        loss = criterion(predicted_targets, targets).to(device)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def train(self, n_episodes=200, max_t=1000, decay=None, verbose=True):
        """Deep Q-Learning
        
        Params
        ======
            n_episodes (int): maximum number of training epsiodes
            max_t (int): maximum number of timesteps per episode
            decay (float): controls the rate of decay of the explore rate
            verbose (bool): whether to print updates on the training process
        
        Returns
        =======
            returns (list[float]): episode returns for analysis of training performance
        """
        returns = [] # list containing total reward from each episode

        # Reasonable default value for explore_rate decay:
        if not decay:
            decay = n_episodes / 11

        for episode in range(1, n_episodes+1):
            explore_rate = self.get_explore_rate(episode, decay)
            state, prob = self.env.reset()
            episode_return = 0

            for t in range(max_t): 
                action = self.policy(state, explore_rate)
                next_state, reward, done, info, prob = self.env.step(action)

                self.memory.add(state, action, reward, next_state, done)
                # If enough samples are available in memory, get random subset and learn:
                if len(self.memory) > self.BATCH_SIZE and t % self.UPDATE_EVERY == 0:
                    experience = self.memory.sample()
                    self.update_Q(experience)
                    self.update_target(self.q_network, self.q_network_target)
                state = next_state
                episode_return += reward
                if done:
                    break
        
            returns.append(episode_return)
            # If verbose mode is switched on, log returns every 10 episodes:
            if verbose and episode % 10 == 0:
                print(f'Episode {episode}\tExplore rate {explore_rate:.2f}\tReturn {episode_return:.2f}')
        
        return returns
```

## Creating of Bioreactor Environment
For this work, we use the chemostat model, which provides a standard description of bioprocess conditions. 
This model is applicable to a wide range of other systems where cell or microorganism growth is important, including wastewater treatment [22] and the gut microbiome [23]. 
Such systems can be especially difficult to control because they are often equipped with minimal online sensors [24], limiting the effectiveness of classical control techniques that are hampered by infrequent or delayed system measurements [20, 25]. 
Hence, the class `BioreactorEnv()` create a chemostat environment that can handle an arbitrary number of bacterial strains where all are being controlled.

### Initialisation
* The variable `xdot` represents an array of the derivatives for all state variables.
* The variable `reward_func` calculate the regard function based on state, action and next state.
* The variable `sampling_time` defines the time between samples and hold intervals.
* The variable `num_controlled_species` its self explanatory.
* The variable `initial_x` represents the initial state (e.g., shape array (8,))
* The variable `max_t` represents the maximum number of timesteps per episode.
* The variable `n_states` represents the number of states (e.g., 10).
* The variable` n_actions` represents the number of actions (e.g., 2).
* The variable `continuous_s` is a boolean flag when `True` returns scaled bacterial populations (1/100000). E.g.,:`[0.26154319 0.2205354 ], otherwise, discretises the population of bacteria to a state suitable for the agent.


### `step(self, action)`
The `step` module performs one sampling and hold interval using the action provided by a reinforcment learning agent, returning state, reward and boolean variable.

### `action_to_u(self,action)`
The `action_to_u` module takes a discrete action index and returns the corresponding continuous `Cin` concentrations of the chosen action.

### `get_state(self)`
The `get_state` module obtains the state, scaled bacterial populations, to be observed by the agent.

### `pop_to_state(self, N)`
The `pop_to_state` module discretises the population of bacteria to a state suitable for the agent.

### `reset(self, initial_x = None)`
The `reset` module resets the environment to its inital state.

```python
class BioreactorEnv():
    '''
    Chemostat environment that can handle an arbitrary number of bacterial strains where all are being controlled
    '''
    def __init__(self, 
                 xdot, 
                 reward_func, 
                 sampling_time, 
                 num_controlled_species, 
                 initial_x, 
                 max_t, 
                 n_states = 10, 
                 n_actions = 2, 
                 continuous_s = False):
        '''
        Parameters:
            xdot: array of the derivatives for all state variables
            reward_func: function to calculate reward: reward = reward_func(state, action, next_state)
            sampling_time: time between sample-and-hold intervals
            num_controlled_species: 2
            initial_x: the initial state (e.g., shape array (8,))
            max_t: maximum number of timesteps per episode
            n_states = 10
            n_actions = 2
            continuous_s=
			True:  get_state self.xs[-1][0:self.num_controlled_species]/100000
			False: get_state = self.pop_to_state(self.xs[-1][0:self.num_controlled_species])
        Returns:
            env returns populations/scaling to agent
        References:
            https://github.com/ucl-cssb/ROCC/blob/master/ROCC/chemostat_env/chemostat_envs.py
        '''
        one_min = 0.016666666667 #(1/60)
        self.scaling = 1 
			#population scaling to prevent neural network instability in agent
			#aim to have pops between 0 and 1. 
        self.xdot = xdot
        self.xs = [] # append odeint solutions of xdot
        self.us = [] # append actions
        self.sampling_time = sampling_time*one_min
        self.reward_func = reward_func

        self.u_bounds = [0,0.1]
        self.N_bounds = [0, 50000]

        self.u_disc = n_actions
        self.N_disc = n_states
        self.num_controlled_species = num_controlled_species
        self.initial_x = initial_x
        self.max_t = max_t
        self.continuous_s = continuous_s
    
    def step(self, action):
        '''
        Performs one sampling and hold interval using the action provided by a reinforcment learning agent

        Parameters:
            action: action chosen by agent
        Returns:
            state: scaled state to be observed by agent
            reward: reward obtained buring this sample-and-hold interval
            done: boolean value indicating whether the environment has reached a terminal state
        '''
        
        u = self.action_to_u(action)
        
        self.us.append(u)

        ts = [0, self.sampling_time]
        sol = odeint(self.xdot, self.xs[-1], ts, args=(u,))[1:]
        self.xs.append(sol[-1,:])
        self.state = self.get_state() #scaled bacterial populations
        reward, done = self.reward_func(self.xs[-1]) #reward func with last appended sol from 0 to max_t
        
        if len(self.xs) == self.max_t:
            done = True

        return self.state, reward, done, None, 1

    def action_to_u(self,action):
        '''
        Takes a discrete action index and returns the corresponding continuous state vector

        Paremeters:
            action: the descrete action
            num_species: the number of bacterial populations
            num_Cin_states: the number of action states the agent can choose from for each species
            Cin_bounds: list of the upper and lower bounds of the Cin states that can be chosen
        Returns:
            state: the continuous Cin concentrations correspoding to the chosen action
        '''

        # calculate which bucket each eaction belongs in
        buckets = np.unravel_index(action, [self.u_disc] * self.num_controlled_species)

        # convert each bucket to a continuous state variable
        u = []
        for r in buckets:
            u.append(self.u_bounds[0] + r*(self.u_bounds[1]-self.u_bounds[0])/(self.u_disc-1))

        u = np.array(u).reshape(self.num_controlled_species,)
        return np.clip(u, self.u_bounds[0], self.u_bounds[1])


    def get_state(self):
        '''
        Gets the state (scaled bacterial populations) to be observed by the agent

        Returns:
            scaled bacterial populations (1/100000). E.g.,:`[0.26154319 0.2205354 ]`
        '''
        if self.continuous_s:
            return self.xs[-1][0:self.num_controlled_species]/100000
        else:
            return self.pop_to_state(self.xs[-1][0:self.num_controlled_species])

    
    def pop_to_state(self, N):
        '''
        discritises the population of bacteria to a state suitable for the agent

        :param N: population
        :return: discitised population
        '''
        step = (self.N_bounds[1] - self.N_bounds[0])/self.N_disc
        N = np.clip(N, self.N_bounds[0], self.N_bounds[1]-1)
        return np.ravel_multi_index((N//step).astype(np.int32), [self.N_disc]*self.num_controlled_species)

    def reset(self, initial_x = None):
        '''
        Resets env to inital state:

        Parameters:
            initial_S (optional) the initial state to be reset to if different to the default
        Returns:
            The state to be observed by the agent
        '''
        
        if initial_x is None:
            initial_x = self.initial_x

        self.xs = [initial_x]
        self.us = []
        return (self.get_state(),1)
```

## Setting up chemostat environment and `DQN_agent` and train `DQN_agent`
To set up your chemostat environment with `BioreactorEnv()`, you need  the following inputs:  
* The `xdot_product` which calculates and returns the derivatives for all state variables with the numerical solver `odeint` in an array (e.g., `xdot` for this work).  
	* The input variables for `xdot_product` are:  
		* x: current state (e.g., xdot.shape = (8,))  
		* t: current time  
		* u: array of the concentrations of the auxotrophic nutrients and the common carbon source  
	* `xdot_product` requires to compute the growth rate based on the `monod` equation (e.g., `growth_rate = ((umax * C) / (Km + C)) * (C0 / (Km0 + C0))`). See (Treloar et al, 2020) for further details.   
* The `reward_function` which is the function to calculate reward: reward = reward_func(state, action, next_state).  
* The `sampling_time` defined as the time between sample-and-hold intervals  
* The `num_controlled_species` which in this case is 2.  
* The `initial_x` defines the initial state (e.g., shape array (8,); `initial_x = np.array([20000, 30000, 0., 0., 1., 0., 0., 0.])`).  
* The `max_t_steps` defined as maximum number of timesteps per episode  
* The `continuous_s` defines the continous state set up to `True` to get the state (scaled bacterial populations) to be observed by the agent.  


### monod equation
```python
def monod(C, C0, umax, Km, Km0):
    '''
    Calculates the growth rate based on the monod equation

    Parameters:
        C: the concetrations of the auxotrophic nutrients for each bacterial
            population
        C0: concentration of the common carbon source
        umax: array of the maximum growth rates for each bacteria
        Km: array of the saturation constants for each auxotrophic nutrient
        Km0: array of the saturation constant for the common carbon source for
            each bacterial species
    '''

    growth_rate = ((umax * C) / (Km + C)) * (C0 / (Km0 + C0))

    return growth_rate


```

### `xdot_product`
```python
def xdot_product(x, t, u):
    '''
    Calculates and returns derivatives for the numerical solver odeint

    Parameters:
        x: current state (e.g., xdot.shape = (8,))
        t: current time
        u: array of the concentrations of the auxotrophic nutrients and the common carbon source
        #num_species: the number of bacterial populations
    Returns:
        xdot: array of the derivatives for all state variables
    References:
        https://github.com/ucl-cssb/ROCC/blob/master/ROCC/chemostat_env/chemostat_envs.py        
    '''
    q = 0.5 #(0-umax) # q term takes account of the dilution
    
    y, y0, umax, Km, Km0 = [np.array(x) for x in [
                            [480000., 480000.], # y (10**12)
                            [520000., 520000.], # y0 (10**12)
                            [1., 1.1], # umax (0.4 - 3)
                            [0.00048776, 0.000000102115],   # Km (2)
                            [0.00006845928, 0.00006845928]]  # Km0 (2)
                           ]
    
    # extract variables
    N = x[:2] #np.array(x[:self.num_species])
    C = x[2:4] #np.array(x[self.num_species:self.num_species+self.num_controlled_species])
    C0 = x[4] # np.array(x[-1])
    A = x[5]
    B = x[6]
    P = x[7]

    R = monod(C, C0, umax, Km, Km0)

    # calculate derivatives
    dC0 = q*(0.1 - C0) - sum(1/y0[i]*R[i]*N[i] for i in range(2)) #Eq1. concentration of the shared carbon source
    dC = q * (u - C) - (1 / y) * R * N # sometimes dC.shape is (2,2); Eq2
    dN = N * (R - q) # Eq4

    dA = N[0] - 2 * A ** 2 * B - q * A
    dB = N[1] - A ** 2 * B - q * B
    dP = A ** 2 * B - q * P

    # construct derivative vector for odeint
    xdot = np.append(dN, dC)
    xdot = np.append(xdot, dC0)
    xdot = np.append(xdot, dA)
    xdot = np.append(xdot, dB)
    xdot = np.append(xdot, dP)
    
    return xdot


```

### Reward function
```python
def reward_function(x):
    """
    calculates the reward based on the rate of product output
    :param x:
    :return:
    """
    P = x[-1]

    if x[0] < 1000 or x[1] < 1000:
        reward = -1
        done = True
    else:
        reward = P/100000
        done = False

    return reward, done
```


### Setting up and train chemostat environment
```python
    num_controlled_species = 2
    sampling_time = 10  # minutes
    max_t_steps = int((24 * 60) / sampling_time)  # set this to 24 hours
    initial_x = np.array([20000, 30000, 0., 0., 1., 0., 0., 0.]) # the initial state

    n_states_env = 2
    n_actions_env = 4

    ## Setting up chemostat environment
    env = BioreactorEnv(xdot_product, 
                        reward_function, 
                        sampling_time,
                        num_controlled_species, 
                        initial_x, 
                        max_t_steps, 
                        #n_states_env, #default: n_states = 10, 
                        #n_actions_env, #default: n_actions = 2, 
                        continuous_s = True)  

    ## Setting up DQN_agent
    n_states = 2
    n_actions = 4
    agent = DQN_agent(env, n_states, n_actions)

    ## Train DQN_agent
    n_episodes = 1000 
    returns = agent.train(n_episodes) # list containing total reward from each episode

    controlling_rate_decay=n_episodes / 11  
    explore_rates = [agent.get_explore_rate(episode, controlling_rate_decay) for episode in range(1, n_episodes+1)]
```


## Results

### Running "chemostat environment" code one time.
The following figures illustrates (A): perfomance of the agent during training, (B): actions of the lowerbound and upperbound of $C_{in}$, (C): final populations curve of $N_{1}$ and $N_{2}$.
During the exploration phase the population levels vary and random actions are taken, as the explore rate decreases they move to the target values (Fig C).

![fig](fig/fig-return_explore_rate.png)  
**Figure A** Return and explore rate. Performance of the agent improves and the explore rate decreases during training.

![fig](fig/fig-actions.png)  
**Figure B** Actions

![fig](fig/fig-population_cells.png)  
**Figure C** Population cells 

### Running "chemostat environment" code one time.
The following plots (Figs A, B and C) illustrate results for running "chemostat environment" five times. 
It is worth noting the stabilisation for final population curves always reach an average values closer to 20000 and 30000.

![fig](fig/tests_results_bioreactor.svg)  
**Figure.** Results for running "chemostat environment" code five times (TEST 00 to TEST 04) using 1000 episodes.

### Peformance and computing
The above results were obtained using `n_episodes = 1000`, taking approximately 25 minutes in a host machine with `33.3G RAM`, and GPU `NVIDIA RTX A200 8192MiB` and codespaces with `2-core • 8GB RAM • 32GB HD`.
Similarly, the following executions times were performed either with a host computer "33.3G RAM, NVIDIA RTX A200 8192MiB" or in CODESPACES  "2-core • 8GB RAM • 32GB HD" with different episodes values.
Note that such execition times might vary for your system.

* Host computer: 33.3G RAM, NVIDIA RTX A200 8192MiB   
	* Execution time (mins): 0.2304505189259847 	 with n_episodes 10  
	* Execution time (mins): 0.4359638055165609 	 with n_episodes 20   
	* Execution time (mins): 0.9316037853558858 	 with n_episodes 50  
	* Execution time (minutes): 1.8308319965998332  with n_episodes 100  
	* Execution time (minutes): 12.566332964102427 with n_episodes 500  
	* Execution time (minutes): 22.788715147972106 with n_episodes 1000  
	* Execution time (minutes): 46.37891451915105 with n_episodes 2000  
* in CODESPACES  2-core • 8GB RAM • 32GB HD   
	* Execution time (mins): 0.3237577478090922 with n_episodes 20   
	* Execution time (mins): 23.73 with n_episodes 1000  

## 6. Conclusions 
We presented how Reinforcement learning, concernig with optimising an agent’s behaviour within an environment, is applied to control two auxotrophic species in a chemostat.
This include introduction to terminology, setting up Deep Q-angent, creating of Bioreactor Environment, setting up chemostat environment and DQN_agent and train DQN_agent, and presenting results with different running and execution times. 
Hence, we demonstrated (A) how to setup a standard description of bioprocess conditions using the chemostat model and (B) the challneges on how the systems can be especially difficult to control with different initial conditions running at multiple times. 

## 7. Assignments 
### 7.1. Demonstrate that the RL agent robust to different initial conditions of the bioreactor environment 
#### 7.1.1 Create population curves of five trained agents controlling the chemostat system with the goal of optimising product output, using default conditions (e.g, [20000, 30000]).
#### 7.1.2 Create population curves of five trained agents controlling the chemostat system with the goal of optimising product output, using these initial conditions (e.g, [30000, 20000]).
#### 7.1.3 Add your conclusions on how the performance of the agent improves or worsened as the explore rate decreases during training. 

## 8. References
[1] Treloar, Neythen J., Alex JH Fedorec, Brian Ingalls, and Chris P. Barnes. "Deep reinforcement learning for the control of microbial co-cultures in bioreactors." PLoS computational biology 16, no. 4 (2020): e1007783. [DOI](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007783); [google-citations](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17698721817212738220)

[5] Croughan, Matthew S., Konstantin B. Konstantinov, and Charles Cooney. "The future of industrial bioprocessing: batch or continuous?." Biotechnology and bioengineering 112, no. 4 (2015): 648-651. [google-citations](https://scholar.google.com/scholar?cites=18103893455237911440&as_sdt=2005&sciodt=0,5&hl=en); [DOI](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/bit.25529)

[6] Shong, Jasmine, Manuel Rafael Jimenez Diaz, and Cynthia H. Collins. "Towards synthetic microbial consortia for bioprocessing." Current Opinion in Biotechnology 23, no. 5 (2012): 798-802. [google-citations](https://scholar.google.com/scholar?cites=1551248015071712309&as_sdt=2005&sciodt=0,5&hl=en); [DOI](https://www.sciencedirect.com/science/article/abs/pii/S0958166912000341)

[26] Sootla, Aivar, Natalja Strelkowa, Damien Ernst, Mauricio Barahona, and Guy-Bart Stan. "Toggling a genetic switch using reinforcement learning." arXiv preprint arXiv:1303.3183 (2013). [google-citations](https://scholar.google.com/scholar?cites=12545971767986048065&as_sdt=2005&sciodt=0,5&hl=en); [arxiv](https://arxiv.org/abs/1303.3183)

[27] Riedmiller, Martin. "Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method." In Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16, pp. 317-328. Springer Berlin Heidelberg, 2005. [google-citations](https://scholar.google.com/scholar?cites=8156369906861868334&as_sdt=2005&sciodt=0,5&hl=en); [DOI](https://doi.org/10.1007/11564096_32)

[28] Lampe, Thomas, and Martin Riedmiller. "Approximate model-assisted neural fitted q-iteration." In 2014 International Joint Conference on Neural Networks (IJCNN), pp. 2698-2704. IEEE, 2014. [google-scholar](https://scholar.google.com/scholar?cites=15116783815853678916&as_sdt=2005&sciodt=0,5&hl=en); [DOI](https://ieeexplore.ieee.org/abstract/document/6889733/)


